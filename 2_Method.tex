\section{Methods}
\label{section:methods}
% Tao: Which classifier did you use?
% Hong Jin: SVM?

As Navigli \shortcite{Navigli09wordsense} noted that supervised approaches have performed best in WSD, we focus on work on supervised approaches. 
%We explored two approaches for performing supervised WSD using word embeddings. Firstly, we experimented with different methods of composing word embeddings to represent the context of a word and used this in conjunction with IMS, a state-of-the-art supervised WSD system. 
%Secondly, we explored the use of Neural Networks, which have produced state-of-the-art performance in many NLP tasks, in performing WSD. A LSTM network, a type of Recurrent Neural Network, is evaluated. 
We explore the use of using word embeddings in IMS. 
In our first set of evaluation, we used tasks from Senseval-2, Senseval-3 and SemEval-2007 to evaluate the performance of our classifiers on monolingual WSD. We do this to first validate that our approach is a sound approach of performing WSD, showing improved or identical scores over state-of-the-art systems in most tasks. 

Similar to the work by Taghipour \shortcite{Taghipour15}, we experimented with the use of word embeddings as feature types in IMS. However, we did not just experiment using C\&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks \cite{schnabel2015evaluation}. We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of Senseval-2 \cite{senseval2-LS-kilgarriff2001} and Senseval-3 \cite{senseval3-LS-mihalcea2004}, we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for Senseval-2 \cite{senseval2-AW-palmer2001} and Senseval-3 \cite{senseval3-AW-snyder2004}, and both the fine \cite{semeval2007-fine-pradhan2007} and coarse-grained \cite{semeval2007-coarse-navigli2007} All Words tasks in SemEval-2007 were used. In order to evaluate our features on the All Words task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus \cite{taghipour2015one}.
To compose word vectors, one baseline method is to sum up the word vectors of the words in the surrounding context or sentence. We primarily experimented on this method of composition, due to its good performance and short training time. For this, every word vector for every lemma in the sentence, excluding the target word, was summed into a context vector, resulting in $d$ features. Stopwords and punctuation are discarded. In Turian's \shortcite{Turian10wordrepresentations} work, two hyperparameters, the capacity (number of dimensions) and size of the word embeddings, were tuned in his experiments. We did the same in our experiments.

As the other features in IMS are binary features, we need to scale down the word embeddings, as suggested by Turian \shortcite{Turian10wordrepresentations}. This is because the range of the word embeddings are not bounded, and therefore can have more influence than binary features. The embeddings are scaled to control their standard deviations. We implemented a variant of this technique as done by Taghipour \shortcite{Taghipour15}, in which we set the target standard deviation for each dimension. A comparison of different values of the scaling parameter, $\sigma$ is done. For each $i \in \{1, 2, .. d\}$:
\\

$E_{i} \leftarrow \sigma \times \frac{E_{i}}{stdev(E_{i})} $, where $\sigma$ is a scaling constant that sets the target standard deviation
\\ 

We evaluate the effect of varying the scaling factor with the feature of the sum of the surrounding word vectors. Word embeddings of 50 dimensions were used.


%\singlespacing

\begin{table}[ht]
	\caption{Effects of varying scaling factor on C\&W embeddings }
	\label{table:wordembeddings-accuracy}
	\begin{center}
		\begin{tabular}{| p{7cm} | p{4cm} | p{4cm} |}
			\hline
			Method & Senseval-2 Accuracy & Senseval-3 Accuracy \\
			\hline
			C\&W, unscaled & 0.569 & 0.641 \\
			\hline
			C\&W, $\sigma _{=0.15}$ & 0.665 & 0.731 \\
			\hline
			C\&W, $\sigma _{=0.1}$ & {\bf0.672} & {\bf0.739} \\
			\hline
			C\&W, $\sigma _{=0.05}$ & 0.664 & 0.735 \\
			\hline
			
		\end{tabular}
	\end{center}
\end{table}
We experimented with different word embeddings and varied the value of $\sigma$. Like Turian \shortcite{Turian10wordrepresentations} and Taghipour \shortcite{Taghipour15}, we found that a value of 0.1 for $\sigma$ worked well, seen in Table \ref{table:wordembeddings-accuracy}. The number of dimensions, known as the capacity, of the word embeddings was tuned by Turian \shortcite{Turian10wordrepresentations}. Hence, we varied the values of the capacity for CBOW and GloVe. As we can see in Tables \ref{table:wordembeddings-word2vec-accuracy} and \ref{table:wordembeddings-glove-accuracy}, in CBOW and GloVe, the context sum feature works optimally with 50 dimensions. 

In Table \ref{table:top-LS}, we compare the performances of our system on Senseval-2 (held in 2001) and Senseval-3's (held in 2004) Lexical Sample tasks with IMS, the top system for each task, and other recent systems that evaluated on the same task.


\begin{table}[ht]
	\caption{Effects of varying capacity on CBOW}
	\label{table:wordembeddings-word2vec-accuracy}
	\begin{center}
		\begin{tabular}{| p{7cm} | p{4cm} | p{4cm} |}
			\hline
			Method & Senseval-2 Accuracy & Senseval-3 Accuracy \\
			\hline
			CBOW, $dimensions_{=50}$ & {\bf0.680} & {\bf0.741} \\
			\hline
			CBOW, $dimensions_{=300}$ & 0.669 & 0.731 \\
			\hline
			
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[ht]
	\caption{Effects of varying capacity on GloVe}
	\label{table:wordembeddings-glove-accuracy}
	\begin{center}
		\begin{tabular}{| p{7cm} | p{4cm} | p{4cm} |}
			\hline
			Method & Senseval-2 Accuracy & Senseval-3 Accuracy \\
			\hline
			GloVe, $dimensions_{=50}$ & {\bf0.678} & {\bf0.741} \\
			\hline
			GloVe, $dimensions_{=100}$ & 0.668 & 0.734 \\
			\hline
			GloVe, $dimensions_{=200}$ & 0.666 & 0.73 \\
			\hline
			
		\end{tabular}
	\end{center}
\end{table}



Because each dimension is a feature that is used by IMS, if there are more dimensions, then there are more features. This may result in over-fitting on small datasets. This is a possible reason that a smaller number of dimensions work better. 

\begin{table}
	\caption{Comparison of systems on Lexical Sample tasks. Rank 1 system refers to the top system for the specified task during the evaluation}
	\label{table:top-LS}
	\begin{center}
		\begin{tabular}{| p{7cm} | p{4cm} | p{4cm} |}
			\hline
			Method & Senseval-2 LS Accuracy & Senseval-3 LS Accuracy \\
			\hline
			IMS + Context Sum, CBOW $\sigma _{=0.1}$ (proposed) & {\bf0.68} & {\bf0.741} \\
			\hline
			
			IMS & 0.653 & 0.726\\
			\hline
			Rank 1 System & 0.642 & 0.729 \\
			\hline
			\newcite{rothe2015autoextend} & 0.666 & 0.736 \\
			\hline
			\newcite{Taghipour15} & 0.662 & 0.734 \\
			\hline
			Most Frequent Sense (Baseline) & 0.476 & 0.552 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}



Apart from the Lexical Sample tasks, we evaluated our systems on the All Words tasks of Senseval-2, Senseval-3 and SemEval-2007, which were evaluated on by Zhong \shortcite{Zhong2010}. As seen in Table \ref{table:All-AW}, our enhancements to IMS to make use of word embeddings give better results on All Words task than the original IMS and the respective Rank 1 Systems during the task. It also outperforms recent systems developed and evaluated in recent papers. We note that although our system increased accuracy on IMS on several tasks, the difference is mostly not statistically significant. 

\begin{table}
	\caption{Accuracy of our system on Senseval-2, Senseval-3, SemEval-2007 All Words task}
	\label{table:All-AW}
	\begin{center}
		\begin{tabular}{| p{4cm} | p{2cm} | p{3cm} | p{3cm} | p{3cm} | }
			\hline
			Method & Senseval-2 AW Accuracy & Senseval-3 AW Accuracy & SemEval-2007 Fine-Grained & SemEval-2007 Coarse-grained \\
			\hline
			IMS + Context Sum, 
			
			CBOW, $\sigma _{=0.1}$ (proposed) & 0.677 & 0.679 & {\bf0.604} & {\bf 0.826 } \\
			\hline
			
			\cite{Taghipour15} & -(unreported) & {\bf0.682} & -(unreported) & -(unreported) \\
			\hline
			\cite{chen2014} & -(unreported) & -(unreported) & -(unreported) & {\bf 0.826 } \\
			\hline
			IMS (on One Million Sense-Tagged dataset) & 0.682 & 0.674 & 0.585 & 0.816 \\
			\hline
			IMS (original) & 0.682 & 0.676 & 0.583 & {\bf 0.826 }  \\
			\hline
			Top System during the task & {\bf0.69} & 0.652 & 0.591 & {\bf 0.825 } \\
			\hline
			WordNet Sense 1 & 0.619 & 0.624 & 0.514 & 0.789\\
			\hline
		\end{tabular}
	\end{center}
\end{table}


It can be seen in Table \ref{table:top-LS} and \ref{table:All-AW} that our enhancements improves the existing IMS system, and we get performance comparable to or better than top approaches in both Lexical Sample tasks and All Words tasks. 

\begin{table}
	\caption{Accuracy of our system on Senseval-2, Senseval-3 Lexical Sample tasks and SemEval-2007 All Words task}
	\label{table:full}
	\begin{center}
\begin{tabular}{|p{1cm}|p{0.5cm}|p{1cm}|p{1cm}|p{2cm}|p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{2cm}|p{2cm}|}
Type & Size & Scaling & Composed by& SE-2 LS Accuracy & SE-3 LS & SE-2 AW & SE-3 AW & SE-2007 Fine grained & SE-2007 Coarse grained &\\
\hline
Senna&50&0.1&Sum&0.671&0.738&0.678&0.673&0.6&0.819 &\\
\hline
Senna&50&0.15&Sum&0.666&0.732&0.675&0.672&0.598&0.817 &\\
\hline
Senna&50&0.05&Sum&0.666&0.734&0.679&0.673&0.594&0.818 &\\
\hline
CBOW&50&0.1&Sum&0.68&0.741&0.677&0.679 &0.604 & 0.826\\
\hline
CBOW&50&0.15&Sum&0.67&0.734&0.673&0.675&0.602&0.827\\
\hline
CBOW&50&0.05&Sum&0.672&0.744&0.68&0.677&0.596&0.822\\
\hline
Glove&50&0.1&Sum&0.679&0.741&0.678&0.68&0.594&0.819 &\\
\hline
Glove&50&0.15&Sum&0.674&0.731&0.68&0.678&0.591&0.819 &\\
\hline
Glove&50&0.05&Sum&0.675&0.738&0.676&0.678&0.596&0.819 &\\
\hline
CBOW&200&0.1&Sum&0.669&0.731&0.676&0.675&0.602&0.82 &\\
\hline
CBOW&200&0.15&Sum&0.651&0.715&0.667&0.673&0.594&0.822 &\\
\hline
CBOW&200&0.05&Sum&0.679&0.742&0.679&0.68&0.602&0.823 &\\
\hline
Glove&200&0.1&Sum&0.666&0.73&0.677&0.679&0.591&0.827 &\\
\hline
Glove&200&0.15&Sum&0.654&0.706&0.674&0.675&0.591&0.826 &\\
\hline
Glove&200&0.05&Sum&0.682&0.741&0.68&0.682&0.6&0.823 &\\
\hline
Senna&50&0.1&Concat&0.659&0.724&0.679&0.674&0.585&0.818 &\\
\hline
CBOW&50&0.1&Concat&0.66&0.725&0.678&0.672&0.581&0.816&\\
\hline
CBOW&200&0.1&Concat&0.667&0.729&0.675&0.67&0.591&0.819&\\
\hline
Glove&50&0.1&Concat&0.657&0.722&0.679&0.671&0.583&0.818&\\
\hline
Glove&200&0.1&Concat&0.664&0.728&0.677&0.669&0.587&0.817&\\
\hline
\end{tabular}
	\end{center}
\end{table}

%\iffalse
\subsection{LSTM Network}

A Long Short Term Memory (LSTM) network is a type of Recurrent Neural Network, that has been shown to have good performance on many NLP classification tasks. Unlike normal Neural Networks which can only accept a fixed size vector as an input, Recurrent neural networks accept variable sized inputs. As such, recurrent neural networks can operate over sequences of word vectors and perform operations on them sequentially. The potential benefit of this approach over our existing approach in IMS is this is that the neural network is able to use information about the sequence of words in classification. Examples of using a neural network for classification are \cite{socher2011parsing}, and \cite{socher2013recursive}. Yuan \shortcite{yuan2016word} explores the use of LSTM networks, using label propagation, for WSD. In our approach, we explore a simpler naive approach instead without the use of label propagation.


For the Lexical Sample tasks, we train the model on the training data provided for the task. For the All Words task, we trained the model on the One Million Sense-Tagged dataset. For each task, similar to IMS, we train a model for each word. 

One problem we encountered with this approach was that there is very few training examples per sense. Although as a whole, the dataset had many training examples, we trained a model for each word, resulting in a fairly small number of relevant training instances for a single word. In particular, rare words had extremely little training data. We obtained the following results:


% For the SVM approach by IMS. Because many of our features are binary features, it's possible to generalise easily. 




\begin{table}
	\caption{Accuracy of our Neural Network approach on the Lexical Sample tasks}
	\label{table:NN-LS}
	\begin{center}
		\begin{tabular}{| p{6cm} | p{4cm} | p{4cm} |}
			\hline
			Method & Senseval-2 Accuracy & Senseval-3 Accuracy \\
			\hline
			LSTM approach (Proposed) & 0.458  & 0.603 \\
			
			\hline
			IMS & 0.653 & {\bf0.726}\\
			\hline
			Rank 1 System during the task & {\bf0.642} & 0.729 \\
			\hline
			Most Frequent Sense (Baseline) & 0.476 & 0.552 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}
	\caption{Accuracy of our Neural Network approach on the All Words tasks}
	\label{table:NN_AW}
	\begin{center}
		\begin{tabular}{| p{7cm} | p{2cm} | p{2cm} | p{2cm} | }
			\hline
			Method & Senseval-2 Accuracy & Senseval-3 Accuracy\\
			\hline
			LSTM approach (Proposed) & 0.619  & 0.623  \\
			
			\hline
			IMS (trained on One Million Sense-Tagged dataset) & 0.682 & {\bf0.674} \\
			\hline
			Rank 1 System during the task & {\bf0.69} & 0.652  \\
			\hline
			Wordnet Sense 1 & 0.619 & 0.624  \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

The performance of the neural network is poor in every task, as we can see in both Tables \ref{table:NN-LS} and \ref{table:NN_AW}. The models converge to just using the most common sense. The reason for this is that WSD suffers from the problem of data sparsity. Although there are many training instances in total, the average amount of training examples for each individual sense is low. 