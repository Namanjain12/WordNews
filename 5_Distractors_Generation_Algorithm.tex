\section{Distractor Generation Component}
\label{sec:distractor}

Assesing mastery over vocabulary is the other key functionality of our
prototype learning platform.  The generation of the multiple choice
selection test requires the selection of alternative choices aside
from the correct answer of the target word.  In this section, we
investigate a way to automatically generate such choices (called
``distractors'') (in English form) for a target word. We postulate ``a
set of suitable distractors'' as: 1) having the same form as the
target word, 2) fitting the sentence's context, and 3) having proper
difficulty level according to user's level of mastery.
%By applying a part-of-speech tagger, we obtain the POS tag for the target word, and then restrict the  %candidate distractors to be selected from the same word class.
As input to the distractor generation algorithm, we provide the target
word, its part-of-speech (obtained by tagging the input sentence
first) and the enclosing webpage's news category. We restrict the
algorithm to produce distractors matching the input POS, and which
match the news category of the page.

%To generate good category-related distractors, it is essential to gather enough words that are more related in a certain category to serve as distractors candidates. By using the approach discussed in Section~\ref{subsec:category}, we crawled more than 1400 articles for seven categories, with around 200 articles in each category. The confidence factor is selected to be 10, which is suitable to classify enough words into different categories. After this step, there should be sufficient “Category-Related" words in each category.

We can design the test to be more difficult by choosing distractors
that are more similar to the target word.  By varying the semantic
distance, we can generate tests at varying difficulty levels.  We
quantify similarity by using the Lin distance~\cite{Lin1998} between two
input candidate concepts in WordNet~\cite{Miller1995}:

\begin{equation}
sim(c1,c2) = \frac{2*log_p(lso(c1,c2))}{log_p(c1)+log_p(c2)}
\label{equation:Distractor_4}
\end{equation}  

\noindent where $p(c)$ denotes the probability of encountering concept
$c$, and $lso(c1,c2)$ denotes the lowest common subsumer synset, which
is the lowest node in the WordNet hierarchy that is a hypernym of both
$c1$ and $c2$.  This returns a score from 0 (completely dissimilar) to
1 (semantically equivalent).

If we use a target word $e$ as the starting point, we can use WordNet
to retrieve related words using WordNet relations (hypernyms/hyponyms,
synonyms/antonyms) and determine their similarity using Lin distance.

We empirically set $0.1$ as the similarity threshold -- words that are
deemed more similar than $0.1$ are returned as possible distractors
for our algorithm.  We note that Lin distance often returns a score of
0 for many pairs and the threshold of $0.1$ allows us to have a large
set of distractors to choose from, while remaining fairly efficient in
run-time distractor generation.

%The selection of threshold value will have a direct effect on the speed of distractors generation process. As a very high threshold value will result in more rounds of calculation in semantic distance calculator, and it will take a long time before the distractors are returned to the front end. After several rounds of analysis of each category’s words and the results returned from semantic distance calculator, the threshold value of 0.1 is selected.

%{\bf Semantic Distance.}
%Before we go to explain the next step, it is essential to introduce the semantic distance calculator we used in the server implementation. 
%
%The perspective of semantic relatedness or its inverse, semantic distance, is a concept that indicates the likeness of two words. It is more general than the concept of similarity as stated in WordNet’s synset relation. Similar entities in WordNet are classified into same synset based on their similarity. However, dissimilar entries may also have a close semantic connection by lexical relationships  such as meronymy (car-wheel) and antonymy (hot-cold), or just by any kind of functional relationship or frequent association(pencil-paper, penguin-Antarctica) \cite{ale01}. Semantic distance calculator aims to calculate the semantic relatedness score between two words.
%
%There are many approaches to calculate semantic relatedness score. In this application, we are using Lin Distance \cite{lin98} to calculate the semantic distance between two concepts. The detail of Lin Distance methodology is explained as follows.
%
%Lin attempted to define a measure of semantic similarity that would be both universal and theoretically justified. There are three intuitions that he used as a basis:
%\begin{itemize}
%\item The similarity between arbitrary objects A and B is related to their commonality; the more commonality they share, the more similar they are;
%\item The similarity between A and B is related to the differences between them; the more differences they have, the less similar they are.
%\item The maximum similarity between A and B is reached when A and B are identical, no matter how much commonality they share. 
%\end{itemize}

%\subsubsection{Distractors Selection Algorithm}

We discretize a learner's knowledge of the word based on their prior
exposure to it.  We then adopt a strategy to generate distractors for
the input word based learners' knowledge level: \\

{\bf Easy}: The learner has been exposed to the word at least $t=$ 3
times.  Two distractors are randomly selected from words that share
the same news category as the target word $e$. The third distractor is
generated using our algorithm. \\
%For the third distractor, the system keeps randomly selecting distractor from the same category, computing %its semantic distance to the target word, and stops until meeting a difficulty one.

{\bf Difficult}: The learner has passed the Easy level test $x=$ 6
times.  All three distractors are generated from the same news
category, using our algorithm.

%; the algorithm will choose distractors solely based on results returned from semantic distance calculator. Similar to the approach when knowledge level is 2, the algorithm will randomly select word from current category’s word list and calculate the semantic distance between the selected word and the target word. If the score is above certain threshold, the selected word is chosen as one of the distractors. The process is continued until the server can find three distractors. 

\subsection{Evaluation}
% Tao: How to choose the third distractor? from d2's 
% Zhao: The three distractors are all selected from the same synset.

The WordGap system~\cite{Knoop2013} represents the most related prior
work on automated distractor generation, and forms our
baseline. WordGap adopts a knowledge-based approach: selecting the
synonyms of synonyms (also computed by WordNet) as distractors.  They
first select the most frequently used word, $w1$, from the target
word's synonym set, and 
%Then we select the synset, let's call it  s1, which the synset where the most frequently used word of w1 lies in. 
then select the synonyms of $w1$, called $s1$.  Finally, WordGap
selects the three most frequently-used words from $s1$ as
distractors.

%There are two evaluations to be done as follows:
%1.  Compare Baseline with Knowledge Level 1 Algorithm
%
%.  Compare Baseline with Knowledge Level 3 Algorithm
%For each comparison, three distractors are generated from the baseline algorithm; three distractors are generated from the stated algorithm in this report. With the first comparison we will be able to see if the category information will help in selecting more suitable distractors. By comparing the results from the both evaluation, we will be able to see if semantic distance and category information will help improve the suitability of distractors.

%he first distractor {\textit d1} is the most frequently used word from target word’s synonym set, and the second distractor {\textit d2} is the most frequently used synonym for  {\textit d1},
%similarly, {\textit d3} BUG (HOW TO GENERATE d3). However, if the number of valid result we can get is less than three, we will choose the word that shares the same antonym with the target word.

%To evaluate the distractors selection strategy as described in this report, we chose the knowledge-based approach used by many other language learning systems, which is to utilize the WordNet data and selection distractors based on synonyms of synonyms. WordGap system uses this approach to generate vocabulary test for its android application.

%In our implementation of the baseline algorithm, we will choose the most frequent used word w1 from the target word’s synonym set, and select the most frequent used word w2 from word w1’s synonym set. The selection process is continued until we can find 3 distractors to form a vocabulary test. However, if the number of valid result we can get is less than 3, we will choose the word that shares the same antonym with the target word.

% Tao: commented
%We study distractors generated for the two extreme cases, {\it i.e.},
%knowledge level 1, and knowledge level 3. Therefore, we conduct a
%pairwise comparison -- K1 vs. Baseline, and K3 vs. Baseline, using the
%same test dataset.

%To compare the two approaches in generating distractors, we designed several survey sets to ask users to compare the plausibility of distractors.

We conducted a human subject evaluation of distractor generation to
assess its fitness for use.  The subjects were asked to rank the
feasibility of a distractor (inclusive of the actual answer) from a
given sentential context.  The contexts were sentences retrieved from
actual news webpages, identical to {\tt SystemA}'s use case.

We randomly selected 50 sentences from recent news articles, choosing
a noun or adjective from the sentence as the target word. We show the
original sentence (leaving the target word as blank) as the context,
and display distractors as choices (see
Figure~\ref{fig:distractor_1}). Subjects were required to read the
sentence and rank the distractors by plausibility: 1 (their answer), 2
(most plausible alternative) to 7 (least plausible alternative).
We recruited 15 subjects from within our institution for the survey.
Half are native English speakers.  
 
We evaluated two scenarios, for two different purposes. In both
evaluations, we generate three distractors using each of the two systems,
and add the original target word for validation (7 options in total,
conforming to our ranking options of 1--7).

Since we have news category information, we wanted to check whether
that information alone could improve distractor generation.
Evaluation~1 tests the WordGap baseline system versus a {\bf Random News}
system that uses random word selection.  It just uses the constraint
that chosen distractors must conform to the news category (be
classified to the news category of the target word).

In our Evaluation~2, we tested our {\bf Hard} setup where our
algorithm is used to generate all distractors against WordGap.  This
evaluation aims to assess the efficacy of our algorithm over the
baseline.


%The evaluation contains 100 questions and is separated into 4 surveys, with each survey containing 25 questions. Each participant is free to choose one or more than one surveys. The purpose is to reduce the workload in each survey to get better responses. The surveys are sent to Year 1 students from School of Computing, National University of Singapore.  There are 15 valid responses with each participant ranking each distractor with a different weight from 1 to 7. Half of the participants are native English speakers.

\begin{figure}[th]
   \centering
   \includegraphics[width=0.45\textwidth]{distractor_new.png}
   \caption{Sample distractor ranking question.}
   \label{fig:distractor_1}
\end{figure}



\subsubsection{Results and Analysis}

Each question was answered by five different users.  We compute the
average ranking for each choice. A lower rating means a more plausible
(harder) distractor.  The rating for all the target words is low
($1.1$ on average) validating their truth and implying that the
subjects answered the survey seriously, assuring the validity of the
evaluation.

For each question, we deem an algorithm to be the winner if its three
distractors as a whole (the sum of three average ratings) are assessed
to be more plausible than the distractors by its competitor. We
calculate the number of wins for each algorithm over the 50 questions
in each evaluation. 

%Each participant’s rank will be the weight of the particular distractor in that question, i.e. if the user rank one distractor as rank “5”, the weight of this distractor in this user’s response will be 5. For each distractor of each question, the ranks of all users’ responses are summed. As the more plausible the distractor is, the higher rank it will have, thus if the sum is higher, the approach is not as plausible as the other from user’s point of view.


\begin{table}[th]
    \caption{WordGap vs. Random News.  Lower scores are better.}
    \label{table:distractor_1}
    \begin{center}
    \begin{tabular}{| p{2.5cm} | p{1.5cm} | p{1.8cm} |}
        \hline
         & {\bf \# of wins} & {\bf Avg. score}\\
        \hline
        WordGap & 27 & 3.84\\
        \hline
        Random News & 23 & 4.10\\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[th]
    \caption{WordGap vs. {\tt SystemA} Hard.  Lower scores are better.}
    \label{table:distractor_2}
    \begin{center}
    \begin{tabular}{| p{2.5cm} | p{1.5cm} | p{1.8cm} |}
        \hline
         & {\bf \# of wins} & {\bf Avg. score}\\
        \hline
        WordGap & 21 & 4.16\\
        \hline
        {\tt SystemA} Hard & 29 & 3.49\\
        \hline
    \end{tabular}
    \end{center}
\end{table}


\begin{table*}[ht]
\caption{Example distractors generated by WordGap and {\tt SystemA} Hard.} 
\label{table:example_distractors}
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
                              & {\bf Distractor} & {\bf News Category} & {\bf Lin Distance} & {\bf Average Rating} \\ \hline
Target Word                   & lark       &               &              & 1.33       \\ \hline
\multirow{3}{*}{WordGap}      & frolic     &               &              & 3.33       \\ \cline{2-5} 
                              & runaround  &               &              & 5.67       \\ \cline{2-5} 
                              & cavort     &               &              & 4.17       \\ \hline
\multirow{3}{*}{{\tt SystemA} Hard} & art        & Entertainment & 0.154        & 1.67       \\ \cline{2-5} 
                              & film       & Entertainment & 0.147        & 3.33       \\ \cline{2-5} 
                              & actress    & Entertainment & 0.217        & 4.83       \\ \hline
\end{tabular}
\end{table*}

We display the results of both evaluations in Table
\ref{table:distractor_1} and Table \ref{table:distractor_2}.  We see
that the WordGap baseline outperforms the random selection,
constrained solely by news category, by 4 wins and a 0.26 lower
average score.  This shows that word news category alone is
insufficient for generating good distractors.  When a target word does
not have a strong category tendency, {\it e.g.}, ``venue'' and
``week'', the random news method cannot select highly plausible
distractors. 

In the second table, our distractor algorithm significantly betters
the baseline in both number of wins (8 more) and average score ($0.67$
lower).  This further confirms that context and semantic information
are complementary for distractor generation. As we mentioned before, a
good distractor should fit the reading context and have a certain
level of difficulty.
Finally, in Table~\ref{table:example_distractors} we show the distractors generated for the target word “lark" in the example survey question (Figure~\ref{fig:distractor_1}).



% Tao: Comments from Min. @Yue, please address
% I think we can generate a few examples of distractors generated by your system as as table that will probably be good, and allow some discussion of the system and motivate a moving threshold.
% For the example sentence that you show in the screenshot (when actually readable), generate some distractors by running your algo several times.
%Put those in a table and add the Lin distance and the News category for the target word.  You can also show the distractors that WordGap generates.
%You can pluck these from the survey so that you can also associate the actual scores given by users.


 

 
 
%If for any question, the sum of weight from all participants for one approach is bigger than the other, then this approach is considered to have won this question. The “average score” is the average sum of weight from each approach for all questions. The lower the average score is, the better performance this approach has gained.

%From Table \ref{table:distractor_1} we can see that in the first comparison, the baseline algorithm actually outscored the knowledge level 1 generation algorithm by 4 questions, with a sum of weight lower than 0.26. From Table \ref{table:distractor_1} we can see that in the second comparison, the knowledge level 3 generation algorithm surpassed the baseline algorithm by 8 questions, with the average weight of 3.49 vs 4.16. 

%\subsubsection{Analysis}
%In knowledge level 1 generation algorithm, there is no semantic distance calculation involved. If the target word to test has no strong category indication, for example, words like 'venue', 'week', it is possible that the knowledge level 1 algorithm will select some distractors that are not as plausible as those coming from the target word's synonym of synonym. 

%However, this problem is solved with the help of semantic distance calculator. In the knowledge level 3 generation algorithm, the distractors chosen are both semantic close and also category-related, which produced a relatively better experiment result.

%Also in the baseline algorithm, it is possible that it will select words that are very rare in real life \cite{sus13}, which may also have influence in the result.
