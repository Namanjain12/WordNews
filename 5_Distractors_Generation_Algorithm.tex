\section{Distractors Generation Algorithm}
\label{sec:distractor}
Assesing mastery over vocabulary is a key functionality in our extension. In this section, we investigate a way to automatically generate suitable distractors (in English form) for a target word. We postulate ``a  set of suitable distractors'' as: 1) having the same form as the target word, 2) fitting the reading context, and 3) having proper difficulty level according to user's level of mastery.
%By applying a part-of-speech tagger, we obtain the POS tag for the target word, and then restrict the  %candidate distractors to be selected from the same word class.
We obtain the POS tag for the target word and restrict the candidate distractors to the same word class.
To make the distractors fit the context, we identify the target word's news category (approach is detailed in Section~\ref{subsec:category}), and select the distractors from the same category.

%To generate good category-related distractors, it is essential to gather enough words that are more related in a certain category to serve as distractors candidates. By using the approach discussed in Section~\ref{subsec:category}, we crawled more than 1400 articles for seven categories, with around 200 articles in each category. The confidence factor is selected to be 10, which is suitable to classify enough words into different categories. After this step, there should be sufficient “Category-Related" words in each category.


The difficulty of a distractor is measured by its {\bf semantic distance} to the target word: the closer it is to the target word, the more difficult the distractor is. To quantify the semantic distance, we employ  Lin's Distance~\cite{lin98} to measure the distance between two words in WordNet~\cite{Miller1995} and define distractors to be difficult if the Lin's Distance score is below some threshold. 
%By observing the generated distractors, 
We empirically set $0.1$ as the threshold.

%The selection of threshold value will have a direct effect on the speed of distractors generation process. As a very high threshold value will result in more rounds of calculation in semantic distance calculator, and it will take a long time before the distractors are returned to the front end. After several rounds of analysis of each category’s words and the results returned from semantic distance calculator, the threshold value of 0.1 is selected.

%{\bf Semantic Distance.}
%Before we go to explain the next step, it is essential to introduce the semantic distance calculator we used in the server implementation. 
%
%The perspective of semantic relatedness or its inverse, semantic distance, is a concept that indicates the likeness of two words. It is more general than the concept of similarity as stated in WordNet’s synset relation. Similar entities in WordNet are classified into same synset based on their similarity. However, dissimilar entries may also have a close semantic connection by lexical relationships  such as meronymy (car-wheel) and antonymy (hot-cold), or just by any kind of functional relationship or frequent association(pencil-paper, penguin-Antarctica) \cite{ale01}. Semantic distance calculator aims to calculate the semantic relatedness score between two words.
%
%There are many approaches to calculate semantic relatedness score. In this application, we are using Lin Distance \cite{lin98} to calculate the semantic distance between two concepts. The detail of Lin Distance methodology is explained as follows.
%
%Lin attempted to define a measure of semantic similarity that would be both universal and theoretically justified. There are three intuitions that he used as a basis:
%\begin{itemize}
%\item The similarity between arbitrary objects A and B is related to their commonality; the more commonality they share, the more similar they are;
%\item The similarity between A and B is related to the differences between them; the more differences they have, the less similar they are.
%\item The maximum similarity between A and B is reached when A and B are identical, no matter how much commonality they share. 
%\end{itemize}
%
%Based on the intuition above, Lin proposed his approach in measuring similarity between two concepts c1, c2 in Equation~\ref{equation:Distractor_4}:
%
%\begin{equation}
%sim(c1,c2) = \frac{2*log_p(lso(c1,c2))}{log_p(c1)+log_p(c2)}
%\label{equation:Distractor_4}
%\end{equation}  
%
%where p(c) denotes the probability of encountering concept c, and lso(c1,c2) denotes the lowest common subsumer, which is the lowest node in WordNet hierarchy that is a hypernym of c1 and c2. 
%
%The distance calculator will return a score from 0 to 1, as can be easily seen from the formula above. If the score is closer to 1, it means the two words are closer in semantic sense. This distance calculator will play an important role in the following algorithm. 

%\subsubsection{Distractors Selection Algorithm}
\subsection{User knowledge Aware Approach}
As previously mentioned, our extension logs user's detailed learning history. We categorize a user's knowledge on a certain word into three levels, based on the number of times that he / she has encountered  the word. Then we adopt different strategies to generate distractors for users in different knowledge levels. 

{\bf Knowledge Level 1 (K1)}: This is the default knowledge level assigned to a user on a new word. Users aren't tested on words where their knowledge level is K1.
%Considering this, our system prefers to generate simple distractors, and thus randomly select three words %from the same news category. 

{\bf Knowledge Level 2 (K2)}: This indicates that the user has known this word for three times. Therefore, the testing is expected to be harder. The first two distractors are randomly selected from those words that share the same news category. For the third distractor, its semantic distance to the target word is checked in addition, making it a more difficult distractor.  
%For the third distractor, the system keeps randomly selecting distractor from the same category, computing %its semantic distance to the target word, and stops until meeting a difficulty one.

{\bf Knowledge Level 3 (K3)}: At this level the user is expected to have a good understanding of the word since she has {\it i.e.}, passed the test six times. Therefore, we make the test even harder, and choose  all three distractors from the same news category along with the semantic distance criteria. 

%; the algorithm will choose distractors solely based on results returned from semantic distance calculator. Similar to the approach when knowledge level is 2, the algorithm will randomly select word from current category’s word list and calculate the semantic distance between the selected word and the target word. If the score is above certain threshold, the selected word is chosen as one of the distractors. The process is continued until the server can find three distractors. 

\subsection{Evaluation}
% Tao: How to choose the third distractor? from d2's 
% Zhao: The three distractors are all selected from the same synset.
To compare with our proposed method, we reimplemented an existing distractor generation method used in WordGap system~\cite{Knoop2013}. WordGap adopts a knowledge-based approach: selecting the synonyms of synonyms (computed in WordNet) as distractors. That is, they select the most frequently used word, w1, from the target word's synonym set. %Then we select the synset, let's call it  s1, which the synset where the most frequently used word of w1 lies in. 
Then they select the synonyms of w1 and call this set as s1.
Synset s1 contains all the words that are synonyms of synonyms of the target word. Finally they select three most frequently used words from s1 as distractors. This we use is as our baseline approach for comparison.

Our proposed method adopts three different strategies to generate distractors according to user's knowledge level. In our evaluation, we study distractors generated for the two extreme cases, {\it i.e.}, knowledge level 1, and knowledge level 3. Therefore, we conduct a pairwise comparison -- K1  vs. Baseline, and K3  vs. Baseline, using the same test dataset.

%There are two evaluations to be done as follows:
%1.  Compare Baseline with Knowledge Level 1 Algorithm
%
%.  Compare Baseline with Knowledge Level 3 Algorithm
%For each comparison, three distractors are generated from the baseline algorithm; three distractors are generated from the stated algorithm in this report. With the first comparison we will be able to see if the category information will help in selecting more suitable distractors. By comparing the results from the both evaluation, we will be able to see if semantic distance and category information will help improve the suitability of distractors.

%he first distractor {\textit d1} is the most frequently used word from target word’s synonym set, and the second distractor {\textit d2} is the most frequently used synonym for  {\textit d1},
%similarly, {\textit d3} BUG (HOW TO GENERATE d3). However, if the number of valid result we can get is less than three, we will choose the word that shares the same antonym with the target word.

%To evaluate the distractors selection strategy as described in this report, we chose the knowledge-based approach used by many other language learning systems, which is to utilize the WordNet data and selection distractors based on synonyms of synonyms. WordGap system uses this approach to generate vocabulary test for its android application.

%In our implementation of the baseline algorithm, we will choose the most frequent used word w1 from the target word’s synonym set, and select the most frequent used word w2 from word w1’s synonym set. The selection process is continued until we can find 3 distractors to form a vocabulary test. However, if the number of valid result we can get is less than 3, we will choose the word that shares the same antonym with the target word.


\subsubsection{User Study}
To compare the two approaches in generating distractors, we ask users to compare the plausibility of distractors.
%To compare the two approaches in generating distractors, we designed several survey sets to ask users to compare the plausibility of distractors.
 We randomly selected 50 sentences from recent news articles and then chose a noun or adjective from the sentence as the target word. In our survey, each question looks like a real MCQ quiz: we show the original sentence (leaving the target word as blank) as the context, and randomly display the six distractors and the target word as choices. Users are required to read the sentence and select the correct answer (that they think) as rating 1, and rank the other choices from 2 (most plausible) to 7 (least plausible) based on their plausibility. Figure~\ref{fig:distractor_1} shows an example survey question.
 
 %In the survey, participants are required to answer each question and rank the plausibility of all distractors from 1 to 7. The correct answer will be ranked as 1, and the least plausible distractor will be ranked as 7. A screenshot of one sample question is shown in Figure \ref{fig:distractor_1}.

We have two tests (K1  vs. Baseline, and K3  vs. Baseline) and each contains 50 questions. We further group 25 questions as one session, and give users the freedom to participate one or more sessions. Each question will be answered by at least five different users.
Finally, we recruited 15 users from our university, and half of them are native English speakers. 
% Tao: please check
In average, each user participate two sessions.



%The evaluation contains 100 questions and is separated into 4 surveys, with each survey containing 25 questions. Each participant is free to choose one or more than one surveys. The purpose is to reduce the workload in each survey to get better responses. The surveys are sent to Year 1 students from School of Computing, National University of Singapore.  There are 15 valid responses with each participant ranking each distractor with a different weight from 1 to 7. Half of the participants are native English speakers.



\begin{figure}[ht]
   \centering
   \includegraphics[width=0.45\textwidth]{distractor_1.jpg}
   \caption{A sample survey question}
   \label{fig:distractor_1}
\end{figure}

\subsubsection{Results and Analysis}
As each question is answered by five different users, we compute the average rating for each choice. A lower rating means a more plausible (harder) distractor. 
Unsurprisingly, the rating for all the target words is low ($1.1$ in average), as they are the ground truth. This implies that the users answered the survey questions seriously, and the evaluation quality is controlled. For each question, we determine a  algorithm to be the winner if its three distractors as a whole (the sum of three average ratings) are more plausible than the distractors by another algorithm. We calculate the number of winning questions for each algorithm and compute the average score across the 50 questions.  Winning more questions, and obtaining a lower average score denotes a better performance for an algorithm.



%Each participant’s rank will be the weight of the particular distractor in that question, i.e. if the user rank one distractor as rank “5”, the weight of this distractor in this user’s response will be 5. For each distractor of each question, the ranks of all users’ responses are summed. As the more plausible the distractor is, the higher rank it will have, thus if the sum is higher, the approach is not as plausible as the other from user’s point of view.

\begin{table}[th]
    \caption{ Baseline vs. Knowledge Level 1}
    \label{table:distractor_1}
    \begin{center}
    \begin{tabular}{| p{1.5cm} | p{2.5cm} | p{2.2cm} |}
        \hline
         & Number of winning questions & Average score\\
        \hline
        Baseline & 27 & 3.84\\
        \hline
        K1 & 23 & 4.10\\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[th]
    \caption{Baseline vs. Knowledge Level 3 }
    \label{table:distractor_2}
    \begin{center}
    \begin{tabular}{| p{1.5cm} | p{2.5cm} | p{2.2cm} |}
        \hline
         & Number of winning questions & Average score\\
        \hline
        Baseline & 21 & 4.16\\
        \hline
        K3 & 29 & 3.49\\
        \hline
    \end{tabular}
    \end{center}
\end{table}

We display the results for Baseline vs. K1 and Baseline vs. K3, in  
Table \ref{table:distractor_1} and Table \ref{table:distractor_2}, respectively.  We see baseline outperforms the K1 algorithm by four more winning questions and 0.26 average score. Recall that, K1 algorithm is solely relied on category information, without taking word semantic relatedness into account. When a target word does not have a strong category tendency, {\it e.g.}, ``venue" and ``week", it is hard for K1 algorithm to select plausible distractors. On the other hand, we see context information ({\it i.e.}, new category) do play a key role, as K1 wins for 23 times.


In Table~\ref{table:distractor_2}, we see our K3 algorithm significantly betters the baseline for both winning questions (8 more) and average score ($0.67$ lower). This further confirms that context and semantic information are complementary for distractor generation, and a good distractor should fit the reading context and have a certain level of difficulty.
 
 


%If for any question, the sum of weight from all participants for one approach is bigger than the other, then this approach is considered to have won this question. The “average score” is the average sum of weight from each approach for all questions. The lower the average score is, the better performance this approach has gained.

%From Table \ref{table:distractor_1} we can see that in the first comparison, the baseline algorithm actually outscored the knowledge level 1 generation algorithm by 4 questions, with a sum of weight lower than 0.26. From Table \ref{table:distractor_1} we can see that in the second comparison, the knowledge level 3 generation algorithm surpassed the baseline algorithm by 8 questions, with the average weight of 3.49 vs 4.16. 

%\subsubsection{Analysis}
%In knowledge level 1 generation algorithm, there is no semantic distance calculation involved. If the target word to test has no strong category indication, for example, words like 'venue', 'week', it is possible that the knowledge level 1 algorithm will select some distractors that are not as plausible as those coming from the target word's synonym of synonym. 

%However, this problem is solved with the help of semantic distance calculator. In the knowledge level 3 generation algorithm, the distractors chosen are both semantic close and also category-related, which produced a relatively better experiment result.

%Also in the baseline algorithm, it is possible that it will select words that are very rare in real life \cite{sus13}, which may also have influence in the result.
